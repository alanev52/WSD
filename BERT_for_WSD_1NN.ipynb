{"cells":[{"cell_type":"markdown","source":["\n","Adopting the initial code by Loureiro et al. (2021) as a starting point, I adapted it extensively to meet our project unique criteria.\n","\n","Link to the original code:  https://github.com/danlou/bert-disambiguation"],"metadata":{"id":"-YOHp0a7bzns"}},{"cell_type":"markdown","source":["#Environment's settings and getting access to the data in Google Drive"],"metadata":{"id":"lNvp4MSPaHI7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2LbOgOBs3aNF"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3DScsVa7IbCn"},"outputs":[],"source":["!pip install transformers\n","!pip install torch torchvision torchaudio\n","!pip install scikit-learn\n","!pip install nltk\n","!pip install numpy\n","!pip install mkl-service\n","!pip install transliterate"]},{"cell_type":"markdown","metadata":{"id":"h5exKljO7RrP"},"source":["# NLM_encoder\n","for RuBERT & multilingual BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3rh3iIkKGVKs"},"outputs":[],"source":["!pip install pymorphy2==0.8"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TxAMN5cO7OWb"},"outputs":[],"source":["import mkl\n","mkl.set_dynamic(0)\n","mkl.set_num_threads(6)\n","\n","import torch as th\n","import numpy as np\n","import pymorphy2\n","from transformers import AutoModel, AutoTokenizer\n","\n","class TransformerEncoder():\n","\n","    def __init__(self, nlm_config):\n","        self.nlm_config = nlm_config\n","        self.nlm_model = None\n","        self.nlm_tokenizer = None\n","\n","        self.load_nlm(nlm_config['model_name_or_path'])\n","        self.morph = pymorphy2.MorphAnalyzer()\n","\n","    def load_nlm(self, model_name_or_path):\n","      self.nlm_model = AutoModel.from_pretrained(model_name_or_path,\n","                                                 output_hidden_states=True)\n","      self.nlm_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n","      self.cls_encoding = self.nlm_tokenizer.encode(self.nlm_tokenizer.cls_token,\n","                                                    add_special_tokens=False)[0]\n","      self.sep_encoding = self.nlm_tokenizer.encode(self.nlm_tokenizer.sep_token,\n","                                                    add_special_tokens=False)[0]\n","      self.pad_encoding = self.nlm_tokenizer.encode(self.nlm_tokenizer.pad_token,\n","                                                    add_special_tokens=False)[0]\n","\n","      self.nlm_model.eval()\n","      self.nlm_model.to('cuda')\n","\n","\n","    def encode_token(self, token):\n","        return self.nlm_tokenizer.encode(token, add_special_tokens=False)\n","\n","\n","    def get_encodings(self, tokens):\n","        return [self.encode_token(t) for t in tokens]\n","\n","\n","    def flatten_encodings(self, encodings):\n","        return sum(encodings, [])\n","\n","\n","    def add_special_encodings(self, encodings):\n","\n","        model_name_or_path = self.nlm_config['model_name_or_path']\n","        return [self.cls_encoding] + encodings + [self.sep_encoding]\n","\n","\n","    def add_padding_encodings(self, encodings, max_len):\n","        encodings += [self.pad_encoding] * (max_len - len(encodings))\n","        return encodings\n","\n","\n","    def get_attention_mask(self, encodings):\n","        att_mask = []\n","        for enc in encodings:\n","            if enc == self.pad_encoding:\n","                att_mask.append(0)\n","            else:\n","                att_mask.append(1)\n","        return att_mask\n","\n","\n","    def merge_subword_embeddings(self, tokens, encodings, embeddings, return_tokens=True):\n","        # align and merge subword embeddings\n","        tok_embeddings = []\n","        encoding_idx = 0\n","        for tok, tok_encodings in zip(tokens, encodings):\n","\n","            if self.nlm_config['subword_op'] == 'mean':\n","                tok_embedding = th.zeros(embeddings.shape[-1]).to('cuda')\n","                for _ in tok_encodings:\n","                    tok_embedding += embeddings[encoding_idx]\n","                    encoding_idx += 1\n","                tok_embedding = tok_embedding / len(tok_encodings)  # avg of subword embs\n","\n","            elif self.nlm_config['subword_op'] == 'first':\n","                tok_embedding = embeddings[encoding_idx]\n","                for _ in tok_encodings:\n","                    encoding_idx += 1  # just move idx\n","\n","            else:\n","                raise(BaseException('Invalid subword_op - %s' % self.nlm_config['subword_op']))\n","\n","            tok_embedding = tok_embedding.detach().cpu().numpy()\n","\n","            if return_tokens:\n","                tok_embeddings.append((tok, tok_embedding))\n","            else:\n","                tok_embeddings.append(tok_embedding)\n","\n","        return tok_embeddings\n","\n","\n","    def get_num_features(self, tokens, n_special_toks=2):\n","        return len(self.get_encodings(tokens)) + n_special_toks\n","\n","\n","    def get_num_subtokens(self, tokens):\n","        return len(self.get_encodings(tokens))\n","\n","\n","    def get_token_embeddings_batch(self, batch_sent_tokens, return_tokens=True):\n","        batch_sent_encodings = [self.get_encodings(sent_tokens) for sent_tokens in batch_sent_tokens]\n","        batch_max_len = max([len(self.flatten_encodings(e)) for e in batch_sent_encodings]) + 2\n","\n","        # prepare nlm input\n","        input_ids, input_mask = [], []\n","        for sent_tokens, sent_encodings in zip(batch_sent_tokens, batch_sent_encodings):\n","\n","            sent_encodings = self.flatten_encodings(sent_encodings)\n","            sent_encodings = self.add_special_encodings(sent_encodings)\n","            sent_encodings = self.add_padding_encodings(sent_encodings, batch_max_len)\n","            input_ids.append(sent_encodings)\n","\n","            sent_attention = self.get_attention_mask(sent_encodings)\n","            input_mask.append(sent_attention)\n","\n","            assert len(sent_encodings) == len(sent_attention)\n","\n","\n","        input_ids = th.tensor(input_ids).to('cuda')\n","        input_mask = th.tensor(input_mask).to('cuda')\n","        with th.no_grad():\n","\n","            if self.nlm_config['model_name_or_path'].startswith('xlnet-'):\n","                pooled, batch_hidden_states = self.nlm_model(input_ids, attention_mask=input_mask)\n","                last_layer = batch_hidden_states[-1]\n","\n","            else:\n","                output_from_model = self.nlm_model(input_ids, attention_mask=input_mask)\n","                last_layer = output_from_model.last_hidden_state\n","                pooled = output_from_model.pooler_output\n","                batch_hidden_states = output_from_model.hidden_states\n","\n","        # select layers of interest\n","        sel_hidden_states = [batch_hidden_states[i] for i in self.nlm_config['layers']]\n","\n","        # merge subword embeddings\n","        merged_batch_hidden_states = []\n","        for layer_hidden_states in sel_hidden_states:\n","            merged_layer_hidden_states = []\n","            for sent_idx, sent_embeddings in enumerate(layer_hidden_states):\n","                sent_embeddings = sent_embeddings[1:-1]  # ignoring special tokens\n","                sent_tokens = batch_sent_tokens[sent_idx]\n","                sent_encodings = batch_sent_encodings[sent_idx]\n","                sent_embeddings = self.merge_subword_embeddings(sent_tokens, sent_encodings, sent_embeddings, return_tokens=return_tokens)\n","                merged_layer_hidden_states.append(sent_embeddings)\n","            merged_batch_hidden_states.append(merged_layer_hidden_states)\n","\n","        # combine layers\n","        combined_batch_embeddings = []\n","        for sent_idx, sent_tokens in enumerate(batch_sent_tokens):\n","            combined_sent_embeddings = []\n","            for tok_idx in range(len(sent_tokens)):\n","                tok_layer_vecs = []\n","                for layer_idx in range(len(merged_batch_hidden_states)):\n","                    tok_layer_vecs.append(merged_batch_hidden_states[layer_idx][sent_idx][tok_idx][1])\n","\n","                if len(tok_layer_vecs) == 1:\n","                    tok_combined_vec = tok_layer_vecs[0]\n","\n","                else:\n","                    tok_layer_vecs = np.array(tok_layer_vecs)\n","\n","                    if self.nlm_config['layer_op'] == 'sum':\n","                        tok_combined_vec = tok_layer_vecs.sum(axis=0)\n","\n","                tok = merged_batch_hidden_states[layer_idx][sent_idx][tok_idx][0]\n","                tok = self.morph.parse(tok)[0].normal_form\n","                combined_sent_embeddings.append((tok, tok_combined_vec))\n","\n","            combined_batch_embeddings.append(combined_sent_embeddings)\n","\n","        return [combined_batch_embeddings]\n","\n","\n","    def token_embeddings(self, batch_sent_tokens, return_tokens=True):\n","        return self.get_token_embeddings_batch(batch_sent_tokens, return_tokens=return_tokens)\n","\n","\n","    def is_valid(self, tokens):\n","        encodings = self.flatten_encodings(self.get_encodings(tokens))\n","        if (len(encodings) + 2) > self.nlm_config['max_seq_len']:\n","            return False\n","        else:\n","            return True\n","\n","\n","if __name__ == '__main__':\n","    \"\"\"For checking NLM_encoder with different models\"\"\"\n","    encoder_cfg = {\n","        'model_name_or_path':'bert-base-multilingual-cased',           #Choices: ['DeepPavlov/rubert-base-cased','bert-base-multilingual-cased']\n","        'weights_path': '',\n","        'min_seq_len': 0,\n","        'max_seq_len': 32,\n","        'layers': [-1, -2, -3, -4],\n","        'layer_op': 'sum',\n","        'subword_op': 'mean'\n","    }\n","\n","    enc = TransformerEncoder(encoder_cfg)\n","    ru_tokenized_s = [['делу', 'время', ',', 'потехе', 'час', '.']]\n","    embs = enc.get_token_embeddings_batch(ru_tokenized_s)\n","    len_embds = len(embs[0])\n","    print(len_embds)\n","    for i in range(len_embds):\n","       for emb in embs[0][i]:\n","        print(emb[0],np.shape(emb[1]))\n"]},{"cell_type":"markdown","metadata":{"id":"OAywwCAtF2q7"},"source":["# WSD_reader\n","\n","for Russain & English"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Elr7xuyt4Fz1"},"outputs":[],"source":["import json\n","from collections import defaultdict\n","import os\n","import pymorphy2\n","from transliterate import translit\n","\n","\n","class WSD_reader():\n","\n","    def __init__(self, path2data):\n","        self.path2data = path2data\n","        self.morph = pymorphy2.MorphAnalyzer()\n","\n","\n","    def load_coarse_senses(self):\n","        senses = defaultdict(list)\n","        with open(self.path2data + '/senses.tsv', 'r', encoding='utf-8' ) as senses_f:\n","            for line in senses_f:\n","                amb_word, sense = line.strip().split('\\t', 1)\n","                senses[amb_word].append(sense)\n","        return dict(senses), list(dict(senses).keys())\n","\n","\n","    def get_word_classes(self, word, setname):\n","        file_path = os.path.join(self.path2data, setname, word, 'classes_map.txt')\n","        with open(file_path) as classes_json_f:\n","            word_classes = json.load(classes_json_f)\n","            return word_classes\n","\n","\n","    def sense2word(self, sense):\n","        return sense.split('_')[0]\n","\n","\n","    def get_sk_mappings(self):\n","        sk_mappings = {}\n","\n","        with open(self.path2data + '/wn_mappings.tsv') as f:\n","            for line_idx, line in enumerate(f):\n","                if line_idx == 0:\n","                    continue\n","                word, coarse_sense, syn_offset, syn_name, sk = line.strip().split('\\t')\n","                sk_mappings[sk] = coarse_sense\n","        return sk_mappings\n","\n","\n","    def load_instances(self, word, split, setname='WSD_full', mode='regular'):\n","        instances = []\n","        with open(self.path2data + '/%s/%s/%s.data.txt' % (setname, word, split),\n","                  'r', encoding=\"utf-8\") as split_data_f:\n","            for line in split_data_f:\n","                word_idx, tokens = line.strip().split('\\t')\n","                word_idx = int(word_idx.lstrip('\\ufeff'))\n","                tokens = tokens.split()\n","                tokens[word_idx] = self.morph.parse(tokens[word_idx])[0].normal_form\n","                custom_mappings = {\n","                    \"zaschita\": \"защита\"\n","                    # Add more custom mappings here as needed\n","                    }\n","                if word in custom_mappings:\n","                    tr_word = custom_mappings[word]\n","                else:\n","                    tr_word = translit(word, \"ru\", reversed=False).lower()\n","\n","                if tokens[word_idx] != tr_word:\n","                    tokens[word_idx] = tr_word\n","                instances.append({'tokens': tokens, 'idx': word_idx, 'class': None})\n","\n","                assert tokens[word_idx] == tr_word\n","\n","        word_classes = self.get_word_classes(word, setname)\n","        mfs_class = word_classes[sorted(list(word_classes.keys()))[0]]   # most frequent sense\n","        lfs_class = word_classes[sorted(list(word_classes.keys()))[-1]]  # least frequent sense\n","        with open(self.path2data + '/%s/%s/%s.gold.txt' % (setname, word, split)) as split_gold_f:\n","            for line_idx, line in enumerate(split_gold_f):\n","                line_class = line.strip()\n","                instances[line_idx]['class'] = word_classes[line_class.lstrip('\\ufeff')]\n","\n","        if mode == 'mfs':\n","            instances = [inst for inst in instances if inst['class'] == mfs_class]\n","        elif mode == 'lfs':\n","            instances = [inst for inst in instances if inst['class'] == lfs_class]\n","        return instances\n","\n","\n","if __name__ == '__main__':\n","    data = WSD_reader('/content/drive/MyDrive/RD_project/MERGED_DATA')\n","    insts = data.load_instances(\"zaschita\", split='train', setname='WSD_full', mode='lfs')\n","    senses, amb_w = data.load_coarse_senses()\n","    print(amb_w)"]},{"cell_type":"markdown","metadata":{"id":"6sLV_7yKj8N9"},"source":["# Creating_1nn_vecs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fVuRJzh0Z0td"},"outputs":[],"source":["import json\n","import os\n","import sys\n","import logging\n","import numpy as np\n","from time import time\n","from transliterate import translit\n","from argparse import Namespace\n","\n","\n","# Define a function to create the vectors\n","def create_vecs(args):\n","    sense_vecs = {}\n","    n_sents = 0\n","    for word in ambiguous_words:\n","        print('Processing \\'%s\\' ...' % word)\n","        word2load = word\n","        for inst_idx, inst in enumerate(data.load_instances\n","         (translit(word2load, \"ru\", reversed=True), split='train', setname=args.dataset_id)):\n","            n_sents += 1\n","            if encoder.get_num_subtokens(inst['tokens']) >= args.max_seq_len:\n","                print('%s:%d exceeds max_seq_len (%d).' % (word, inst_idx, args.max_seq_len))\n","                continue\n","            try:\n","                inst_vecs = encoder.token_embeddings([inst['tokens']])[0][0]\n","            except:\n","                print('ERROR: %s:%d' % (word, inst_idx + 1))\n","                continue\n","\n","            assert inst_vecs[inst['idx']][0] == word  # sanity check\n","\n","            word_vec = inst_vecs[inst['idx']][1]\n","            word_cls = inst['class']\n","\n","            try:\n","                sense_vecs[word_cls]['vecs_sum'] += word_vec\n","                sense_vecs[word_cls]['vecs_num'] += 1\n","            except KeyError:\n","                sense_vecs[word_cls] = {'vecs_sum': word_vec, 'vecs_num': 1}\n","\n","    print('Writing Sense Vectors to %s ...' % args.out_path)\n","    output_file = os.path.join(args.out_path, args.dataset_id, args.nlm_id + '.txt')\n","    output_directory = os.path.dirname(output_file)\n","    os.makedirs(output_directory, exist_ok=True)\n","    with open(output_file , 'w') as vecs_f:\n","        for sense, vecs_info in sense_vecs.items():\n","            vec = vecs_info['vecs_sum'] / vecs_info['vecs_num']\n","            vec_str = ' '.join([str(round(v, 6)) for v in vec.tolist()])\n","            vecs_f.write('%s %s\\n' % (sense, vec_str))\n","\n","\n","if __name__ == '__main__':\n","    args = Namespace(\n","        nlm_id= 'bert-base-multilingual-cased',  # choices: ['DeepPavlov/rubert-base-cased','bert-base-multilingual-cased']\n","        dataset_id= 'WSD_full',\n","        max_seq_len=512,\n","        subword_op='mean',\n","        layers='-1 -2 -3 -4',\n","        layer_op='sum',\n","        out_path='/content/drive/MyDrive/RD_project/ru_mBERT_f_ex'\n","    )\n","\n","    args.layers = [int(n) for n in args.layers.split(' ')]\n","\n","    encoder_cfg = {\n","        'model_name_or_path': args.nlm_id,\n","        'min_seq_len': 0,\n","        'max_seq_len': args.max_seq_len,\n","        'layers': args.layers,\n","        'layer_op': 'sum',\n","        'subword_op': 'mean'\n","    }\n","\n","    print('Loading NLM ...')\n","    encoder = TransformerEncoder(encoder_cfg)\n","    data = WSD_reader('/content/drive/MyDrive/RD_project/MERGED_DATA')\n","    senses, ambiguous_words = data.load_coarse_senses()\n","    create_vecs(args)"]},{"cell_type":"markdown","source":["# Evaluation"],"metadata":{"id":"jXElp9VXYGWd"}},{"cell_type":"code","execution_count":11,"metadata":{"id":"IUQkqaJdtxPV","executionInfo":{"status":"ok","timestamp":1701102642309,"user_tz":-60,"elapsed":508,"user":{"displayName":"Anastasia Alexandrova","userId":"03869832558556348480"}}},"outputs":[],"source":["import numpy as np\n","\n","\n","class VSM(object):\n","    def __init__(self, vecs_path, normalize=True):\n","        self.labels = []\n","        self.vectors = np.array([], dtype=np.float32)\n","        self.indices = {}\n","        self.ndims = 0\n","        self.load_txt(vecs_path)\n","        if normalize:\n","            self.normalize()\n","\n","    def load_txt(self, vecs_path):\n","        self.vectors = []\n","        with open(vecs_path, encoding='utf-8') as vecs_f:\n","            for line in vecs_f:\n","                elems = line.split()\n","                self.labels.append(elems[0])\n","                self.vectors.append(np.array(list(map(float, elems[1:])), dtype=np.float32))\n","        self.vectors = np.vstack(self.vectors)\n","        self.indices = {l: i for i, l in enumerate(self.labels)}\n","        self.ndims = self.vectors.shape[1]\n","\n","    def normalize(self):\n","        self.vectors = (self.vectors.T / np.linalg.norm(self.vectors, axis=1)).T  # L2\n","\n","    def get_vec(self, label):\n","        return self.vectors[self.indices[label]]\n","\n","    def similarity(self, label1, label2):\n","        v1 = self.get_vec(label1)\n","        v2 = self.get_vec(label2)\n","        return np.dot(v1, v2).tolist()\n","\n","    def most_similar_vec(self, vec, topn=100):\n","        sims = np.dot(self.vectors, vec)\n","        sims_ = sims.tolist()\n","        r = []\n","        for top_i in sims.argsort().tolist()[::-1][:topn]:\n","            r.append((self.labels[top_i], sims_[top_i]))\n","        return r\n","\n","    def sims(self, vec):\n","        return np.dot(self.vectors, np.array(vec)).tolist()\n","\n","if __name__ == \"__main__\":\n","   vsm = VSM(\"/content/drive/MyDrive/RD_project/ru_deepPavlov_f_ex/WSD_full/DeepPavlov/rubert-base-cased.txt\")\n","   vectors = vsm.load_txt('/content/drive/MyDrive/RD_project/ru_deepPavlov_f_ex/WSD_full/DeepPavlov/rubert-base-cased.txt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X_UEEaDWtX4D"},"outputs":[],"source":["import argparse\n","import json\n","from time import time\n","from datetime import datetime\n","from collections import defaultdict\n","from collections import Counter\n","from argparse import Namespace\n","import numpy as np\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","from transliterate import translit\n","\n","\n","custom_mappings = {\n","    \"засчита\": \"защита\"\n","    # Add more custom mappings here as needed\n","}\n","\n","def eval_nn(args):\n","    all_sense_preds = defaultdict(list)\n","    all_results = defaultdict(list)\n","    GOLD_test = defaultdict(list)\n","    PREDICTIONS = defaultdict(list)\n","    data = WSD_reader('/content/drive/MyDrive/RD_project/MERGED_DATA')\n","    senses, amb_w = data.load_coarse_senses()\n","\n","    # matching test instances\n","    for amb_word in amb_w:\n","        print('Evaluating %s ...' % amb_word)\n","        for inst_idx, test_inst in enumerate(data.load_instances(translit(amb_word, \"ru\", reversed=True), split='test', setname=args.dataset_id, mode=args.mode)):\n","            gold_sense = test_inst['class']\n","\n","            if encoder.get_num_subtokens(test_inst['tokens']) >= args.max_seq_len:\n","                print('%s:%d exceeds max_seq_len (%d).' % (amb_word, inst_idx, args.max_seq_len))\n","\n","                preds = [('NULL', -1)]\n","                all_sense_preds[gold_sense].append(preds)\n","                all_results[amb_word].append((test_inst, preds))\n","                continue\n","\n","            inst_vecs = encoder.token_embeddings([test_inst['tokens']])[0][0]\n","\n","            assert inst_vecs[test_inst['idx']][0] == amb_word  # sanity check\n","\n","            amb_word_vec = inst_vecs[test_inst['idx']][1]\n","            amb_word_vec = amb_word_vec / np.linalg.norm(amb_word_vec)\n","\n","            preds = senses_vsm.most_similar_vec(amb_word_vec, topn=None)\n","\n","            # filter preds for target word\n","            word =  translit(amb_word, \"ru\", reversed=True)\n","            if word in custom_mappings:\n","                word = custom_mappings[word]\n","            preds = [(sense, score) for sense, score in preds if sense.split('_')[0]\n","                     == word]\n","\n","            all_sense_preds[gold_sense].append(preds)\n","            all_results[amb_word].append((test_inst, preds))\n","            GOLD_test[amb_word].append(gold_sense)\n","            PREDICTIONS[amb_word].append(max(preds, key=lambda x: x[1])[0])\n","    # computing accuracies\n","    all_senses_accs = {}\n","    all_words_accs  = {}\n","    for amb_word in senses:\n","        n_word_correct, n_word_insts = 0, 0\n","        all_gold, all_pred = [], []\n","        for sense in senses[amb_word]:\n","            sense_preds = all_sense_preds[sense]\n","            if len(sense_preds) == 0:\n","                continue\n","            n_sense_correct = sum([1 for preds in sense_preds if preds[0][0] == sense])\n","            sense_acc = n_sense_correct / len(sense_preds)\n","            all_senses_accs[sense] = sense_acc\n","\n","            n_word_correct += n_sense_correct\n","            n_word_insts += len(sense_preds)\n","\n","            all_pred += [preds[0][0] for preds in sense_preds]\n","            all_gold += [sense] * len(sense_preds)\n","\n","        word_recall_scores = recall_score(all_gold, all_pred, average=None)\n","        word_recall_MFS = word_recall_scores[0]\n","        word_recall_LFS = word_recall_scores[-1]\n","\n","        word_precision_scores = precision_score(all_gold, all_pred, average=None, zero_division=0)\n","        word_precision_MFS = word_precision_scores[0]\n","        word_precision_LFS = word_precision_scores[-1]\n","\n","        all_words_accs[amb_word] = n_word_correct / n_word_insts\n","    # writing summary\n","    if args.mode != 'regular':\n","        summary_path = '/content/drive/MyDrive/RD_project/results/%s/1nn/%s/summary_f1.%s.csv' % (args.dataset_id, args.nlm_id, args.mode)\n","    else:\n","        summary_path = '/content/drive/MyDrive/RD_project/results/%s/1nn/%s/summary_f1.csv' % (args.dataset_id, args.nlm_id)\n","\n","    directory = os.path.dirname(summary_path)\n","    # Create the directory if it doesn't exist\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","\n","    with open(summary_path, 'w') as summary_f:\n","        summary_f.write('word,sense,n_insts,f1_micro,f1_weighted,f1_macro\\n')\n","        for amb_word in senses:\n","            n_word_insts = 0\n","            for i, sense in enumerate(senses[amb_word]):\n","                if sense not in all_senses_accs:\n","                    continue\n","                word_f1_macro = np.round(f1_score(GOLD_test[amb_word],\n","                                                  PREDICTIONS[amb_word],\n","                                                  average='macro'),3)\n","                word_f1_weighted = np.round(f1_score(GOLD_test[amb_word],\n","                                                     PREDICTIONS[amb_word],\n","                                                     average='weighted'),3)\n","                word_f1_none = np.round(f1_score(GOLD_test[amb_word],\n","                                                 PREDICTIONS[amb_word],\n","                                                 average=None),3)\n","                sense_acc = all_senses_accs[sense]\n","                s_f1_macro = list(word_f1_none)[i]\n","                n_sense_insts = len(all_sense_preds[sense])\n","                n_word_insts += n_sense_insts\n","                summary_f.write('%s,%s,%d,%f,%f,%f\\n' % (amb_word,sense,\n","                                                         n_sense_insts,\n","                                                         sense_acc,\n","                                                         sense_acc,\n","                                                         s_f1_macro))\n","\n","            word_acc = all_words_accs[amb_word]\n","            summary_f.write('%s,%s,%d,%f,%f,%f\\n' % (amb_word,'ALL',\n","                                                     n_word_insts,word_acc,\n","                                                     word_f1_weighted,\n","                                                     word_f1_macro))\n","\n","\n","    # store full results for further analysis\n","    for amb_word in all_results:\n","\n","        if args.mode != 'regular':\n","            word_results_path = '/content/drive/MyDrive/RD_project/results/%s/1nn/%s/%s.%s.jsonl' % (args.dataset_id, args.nlm_id, amb_word, args.mode)\n","        else:\n","            word_results_path = '/content/drive/MyDrive/RD_project/results/%s/1nn/%s/%s.jsonl' % (args.dataset_id, args.nlm_id, amb_word)\n","\n","        with open(word_results_path, 'w') as word_results_f:\n","            for inst_idx, (test_inst, inst_matches) in enumerate(all_results[amb_word]):\n","                jsonl_results = {'idx': inst_idx, 'matches': inst_matches, 'gold': test_inst['class'], 'tokens': test_inst['tokens']}\n","                word_results_f.write('%s\\n' % json.dumps(jsonl_results,ensure_ascii=False, sort_keys=True))\n","\n","\n","if __name__ == '__main__':\n","    args = Namespace(\n","        nlm_id='DeepPavlov/rubert-base-cased',      #choice = ['bert-base-multilingual-cased', 'DeepPavlov/rubert-base-cased']\n","        dataset_id='WSD_full',\n","        sv_path ='/content/drive/MyDrive/RD_project/ru_deepPavlov_f_ex/WSD_full/DeepPavlov/rubert-base-cased.txt',\n","        subword_op='mean', #choices=['mean', 'first', 'sum']\n","        layers='-1 -2 -3 -4',\n","        max_seq_len = 512,\n","        layer_op='sum',  # choices=['mean', 'first', 'sum'])\n","        mode = 'regular' # choices=['regular', 'mfs', 'lfs']\n","    )\n","\n","    args.layers = [int(n) for n in args.layers.split(' ')]\n","    # catch mismatched nlms/sense_vecs\n","    if args.nlm_id.split('/')[-1] not in args.sv_path.split('/')[-1].split('.'):\n","        logging.fatal(\"Provided sense vectors don't seem to match nlm_id (%s).\" % args.nlm_id)\n","        raise SystemExit('Fatal Error.')\n","\n","    encoder_cfg = {\n","        'model_name_or_path': args.nlm_id,\n","        'min_seq_len': 0,\n","        'max_seq_len': args.max_seq_len,\n","        'layers': args.layers,\n","        'layer_op': args.layer_op,\n","        'subword_op': args.subword_op\n","    }\n","\n","    print('Loading NLM ...')\n","    encoder = TransformerEncoder(encoder_cfg)\n","\n","    print('Loading VSM ...')\n","    senses_vsm = VSM(args.sv_path, normalize=True)\n","\n","    eval_nn(args)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1_p9LJtvhJstm7jZqSWwVLmJOiAnpSQsy","authorship_tag":"ABX9TyOo0WkOT9K2/L5sKHkrs9nI"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}