{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPbMGSJbmmm6rxvFWO0ZwBY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# BERT for WSD with Fine-tuning\n","Building upon the groundwork laid by Loureiro et al. (2021), we tailored the code to better integrate with Russian context.\n","\n","Link to the original code and article: https://github.com/danlou/bert-disambiguation"],"metadata":{"id":"_K536Pg9FiTD"}},{"cell_type":"markdown","source":["# Environment Settings and Access to Data"],"metadata":{"id":"O7QVwpB6E67b"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"hqXaFJ3Zi8YO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers\n","!pip install transliterate"],"metadata":{"id":"Ft5A0D4qjO5Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Classification Task: Refining Embeddings with Fine-Tuning"],"metadata":{"id":"n1nJGLBOv86Y"}},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"N68jBy9aEbFw"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","\n","class MaskedAverageLayer(nn.Module):\n","    def __init__(self):\n","      super(MaskedAverageLayer, self).__init__()\n","\n","    def forward(self, seq, mask):\n","        '''\n","        Inputs:\n","            -seq : Tensor of shape [B, T, E] containing embeddings of sequences\n","            -mask : Tensor of shape [B, T, 1] containing masks to be used to pull from seq\n","        '''\n","        output = None\n","        if mask is not None:\n","          if len(mask.shape) < len(seq.shape):\n","            mask = mask.unsqueeze(-1)\n","            mask = mask.repeat(1, 1, seq.shape[-1])\n","\n","          masked_inputs = (mask.int()*seq) + (1-mask.int())*torch.zeros_like(seq)\n","          unmasked_counts = torch.sum(mask.float(), dim=1)\n","          output = torch.sum(masked_inputs, dim=1)/(unmasked_counts+1e-10)\n","\n","        return output"],"metadata":{"id":"XtQH2RGVjHmf","executionInfo":{"status":"ok","timestamp":1702379699876,"user_tz":-60,"elapsed":234,"user":{"displayName":"Anastasia Alexandrova","userId":"03869832558556348480"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from transformers import AutoModel\n","\n","\n","class GeneralModel(nn.Module):\n","\n","    def __init__(self, model_name, pretrained_path, class_count, freeze_bert=False):\n","        super().__init__()\n","        self.pretrained = AutoModel.from_pretrained(model_name, cache_dir=pretrained_path)\n","        if freeze_bert:\n","            for p in self.bert_layer.parameters():\n","                p.requires_grad = False\n","        self.masked_average = MaskedAverageLayer()\n","        self.linear = nn.Linear(2 * self.pretrained.config.hidden_size, class_count)\n","\n","    def forward(self, seq, attn_masks, target_mask):\n","        '''\n","        Inputs:\n","            -seq : Tensor of shape [B, T] containing token ids of sequences\n","            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n","            -target_mask : target word mask\n","        '''\n","\n","        outputs = self.pretrained(seq, attention_mask=attn_masks)\n","        embeddings = outputs.last_hidden_state\n","        masked_average = self.masked_average(embeddings, target_mask)\n","        cls = embeddings[:, 0]\n","        combined = torch.cat((cls, masked_average), 1)\n","        logits = self.linear(combined)\n","        return logits"],"metadata":{"id":"V1sL6k-DjMxQ","executionInfo":{"status":"ok","timestamp":1702379570901,"user_tz":-60,"elapsed":10,"user":{"displayName":"Anastasia Alexandrova","userId":"03869832558556348480"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Model Configuration"],"metadata":{"id":"foeaRfEJEPtZ"}},{"cell_type":"code","source":["config = {\n","\"PATH\": {\n","        \"tokenizer_path\": \"/content/bert_tokenizer\",\n","        \"pre_trained_path\": \"/content/bert_model\",\n","        \"data_path\": \"/content/drive/MyDrive/RD_project/MERGED_DATA/WSD_full\",\n","        \"output_path\": \"/content/drive/MyDrive/RD_project/output3/\",\n","\t\"model_save_path\": \"/content/ruBERT_trained_models\"\n","\t},\n","\n","\"HYPER_PARAM\" : {\n","                \"learning_rate\": 2e-5,\n","                \"batch_size\": 4,\n","                \"epochs\": 20,\n","                \"sequence_length\": 128\n","                },\n","\n","\"TARGET_DATASETS\" : [\n","                    'dejstvie',\n","                    'delo',\n","                    \"den'\",\n","                    'disk',\n","                    'dokument',\n","                    'dolja',\n","                    'dom',\n","                    'doroga',\n","                    'duh',\n","                    'mesto',\n","                    'moment',\n","                    'pravo',\n","                    \"set'\",\n","                    'sistema',\n","                    \"stat'ja\",\n","                    'vremja',\n","                    'zadacha',\n","                    'zakon',\n","                    'zaschita',\n","                    'zemlja',\n","                    \"zhizn'\"\n","                    ]\n","}"],"metadata":{"id":"zChErzZhjk5_","executionInfo":{"status":"ok","timestamp":1702379570902,"user_tz":-60,"elapsed":9,"user":{"displayName":"Anastasia Alexandrova","userId":"03869832558556348480"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Preprocessing Data\n","\n","We lemmatize target words in each sample for consistency"],"metadata":{"id":"SvetYArGEHfK"}},{"cell_type":"code","source":["import os\n","import json\n","from transliterate import translit\n","\n","\n","def parse_raw_data(data_path):\n","    data = {}\n","    word = data_path.split('/')[-1]\n","    custom_mappings = {\n","                  \"zaschita\": \"защита\"\n","                  }\n","    if word in custom_mappings:\n","        tr_word = custom_mappings[word]\n","    else:\n","        tr_word = translit(word, \"ru\", reversed=False).lower()\n","    with open(data_path + \"/train.data.txt\", \"r\") as f:\n","\n","        data[\"train\"] = []\n","        lines = f.readlines()\n","        for line in lines:\n","            index = int(line.lstrip('\\ufeff').split(\"\\t\")[0])\n","            text = line.split(\"\\t\")[1][:-1]\n","            tokens = text.split()\n","\n","            if tokens[index] != tr_word:\n","                tokens[index] = tr_word\n","            else:\n","                tr_word = translit(tokens[index], \"ru\", reversed=False).lower()\n","            text = ' '.join(tokens)\n","            data[\"train\"].append({\"target_index\": index, \"text\": text})\n","\n","    # train gold\n","    with open(data_path + \"/train.gold.txt\", \"r\") as f:\n","        lines = f.readlines()\n","        for i, line in enumerate(lines):\n","            label = int(line.lstrip('\\ufeff').split(\"\\n\")[0])\n","            data[\"train\"][i][\"label\"] = label\n","\n","    # test data\n","    with open(data_path + \"/test.data.txt\", \"r\") as f:\n","        data[\"test\"] = []\n","        lines = f.readlines()\n","        for line in lines:\n","\n","            index = int(line.lstrip('\\ufeff').split(\"\\t\")[0])\n","            text = line.split(\"\\t\")[1][:-1]\n","            tokens = text.split()\n","\n","            if tokens[index] != tr_word:\n","                tokens[index] = tr_word\n","            else:\n","                tr_word = translit(tokens[index], \"ru\", reversed=False).lower()\n","            text = ' '.join(tokens)\n","            data[\"test\"].append({\"target_index\": index, \"text\": text})\n","\n","    # test gold\n","    with open(data_path + \"/test.gold.txt\", \"r\") as f:\n","        lines = f.readlines()\n","        for i, line in enumerate(lines):\n","            label = int(line.lstrip('\\ufeff').split(\"\\n\")[0])\n","            data[\"test\"][i][\"label\"] = label\n","\n","    # class map\n","    with open(data_path + \"/classes_map.txt\", \"r\") as f:\n","        data[\"class_map\"] = json.loads(f.read())\n","\n","    return data\n","\n","if __name__==\"__main__\":\n","  ru_data = parse_raw_data('/content/drive/MyDrive/RD_project/MERGED_DATA/WSD_full/zaschita')\n","  for _ in ru_data.items():\n","     print(_)"],"metadata":{"id":"PVbubLT5j47A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DataLoader"],"metadata":{"id":"ZiSY7u5oEBQR"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import TensorDataset, SequentialSampler, DataLoader\n","\n","\n","class GeneralModelInput (object):\n","\n","    def __init__(self, sequence_length, batch_size, tokenizer, parsed_data):\n","        self.tokenizer = tokenizer\n","        self.parsed_data = parsed_data\n","        self.batch_size = batch_size\n","        self.sequence_length = sequence_length\n","\n","    def torch_data(self):\n","        data = {\"input_ids\": [],\n","                  \"attention_masks\": [],\n","                  \"target_masks\": [],\n","                  \"labels\": []}\n","        for i, sample in enumerate(self.parsed_data):\n","            encoded_dict = self.tokenizer.encode_plus(\n","                sample[\"text\"],\n","                add_special_tokens=True,\n","                max_length=self.sequence_length,\n","                padding=\"max_length\",\n","                truncation = True,\n","                return_attention_mask=True,\n","                return_tensors='pt'\n","            )\n","            data[\"input_ids\"].append(encoded_dict['input_ids'])\n","            data[\"attention_masks\"].append(encoded_dict['attention_mask'])\n","\n","            words = sample[\"text\"].split()\n","            taget_mask = [False]\n","            for word_index, word in enumerate(words):\n","                tokenized_len = len(self.tokenizer.encode_plus(word)['input_ids']) - 2\n","                if sample[\"target_index\"] == word_index:\n","\n","                    taget_mask += [True] * tokenized_len\n","                else:\n","                    taget_mask += [False] * tokenized_len\n","            taget_mask += [False] * (self.sequence_length - len(taget_mask))\n","            data[\"target_masks\"].append(taget_mask[:self.sequence_length])\n","\n","            data[\"labels\"].append(sample[\"label\"])\n","\n","        data[\"input_ids\"] = torch.cat(data[\"input_ids\"], dim=0)\n","        data[\"attention_masks\"] = torch.cat(data[\"attention_masks\"], dim=0)\n","        data[\"target_masks\"] = torch.tensor(data[\"target_masks\"])\n","        data[\"labels\"] = torch.tensor(data[\"labels\"])\n","\n","        result = DataLoader(TensorDataset(data[\"input_ids\"],\n","                                              data[\"attention_masks\"],\n","                                              data[\"target_masks\"],\n","                                              data[\"labels\"]),\n","                            batch_size=self.batch_size)\n","\n","        return result\n","\n","if __name__ == \"__main__\":\n","  from transformers import AutoTokenizer\n","  parsed_data = parse_raw_data('/content/drive/MyDrive/RD_project/WSD_data/WSD_1/delo')\n","  model_name = \"bert-base-uncased\"                                              #choices: [\"bert-base-multilingual-uncased\", \"DeepPavlov/rubert-base-cased\"]\n","  tokenizer =  AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","  model = GeneralModel(model_name, config[\"PATH\"][\"pre_trained_path\"],len(parsed_data[\"class_map\"])).to('cuda')\n","  train_data = GeneralModelInput (128, 4, tokenizer, parsed_data[\"train\"]).torch_data()\n","  for batch in train_data:\n","    input_ids, attention_masks, target_masks, labels = batch\n","    # Print the batch details\n","    print(\"Batch input_ids shape:\", input_ids.dtype, input_ids.shape)\n","    print(\"Batch attention_masks shape:\", attention_masks.dtype, attention_masks.shape)\n","    print(\"Batch target_masks shape:\", target_masks.dtype, target_masks.shape)\n","    print(\"Batch labels shape:\", labels.dtype, labels.shape)\n"],"metadata":{"id":"qudSuxwHkGdu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Accuracy Function"],"metadata":{"id":"zFiyfB8BEhnM"}},{"cell_type":"code","source":["import numpy as np\n","\n","\n","def accuracy_from_logits(logits, labels):\n","    np_logits = logits.detach().cpu().numpy()\n","    np_labels = labels.detach().cpu().numpy()\n","\n","    total = len(labels)\n","    correct = np.sum((np.argmax(np_logits, axis=1) == np_labels).astype('int'))\n","    return float(correct) / total"],"metadata":{"id":"5LucMDeakdau"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training & Evaluating"],"metadata":{"id":"oI-Vr374EttX"}},{"cell_type":"code","source":["import os\n","import json\n","import argparse\n","import torch.nn as nn\n","import torch.optim as optim\n","from transformers import AutoTokenizer\n","import numpy as np\n","import torch\n","\n","\n","model_name = \"bert-base-uncased\"                                                # choices: [\"DeepPavlov/rubert-base-cased\", \"bert-base-multilingual-uncased\"]\n","save_model = False\n","for target_dataset in config[\"TARGET_DATASETS\"]:\n","    print(\"\")\n","    print (target_dataset)\n","    parsed_data = parse_raw_data(config[\"PATH\"][\"data_path\"]+\"/\"+target_dataset)\n","\n","    if model_name in [\"bert-base-multilingual-uncased\", \"DeepPavlov/rubert-base-cased\", \"bert-base-uncased\" ]:\n","        tokenizer =  AutoTokenizer.from_pretrained(model_name, do_lower_case=True, cache_dir=config[\"PATH\"][\"tokenizer_path\"])\n","        model = GeneralModel(model_name, config[\"PATH\"][\"pre_trained_path\"], len(parsed_data[\"class_map\"])).to('cuda:0')\n","        train_data = GeneralModelInput (config[\"HYPER_PARAM\"][\"sequence_length\"],\n","                                config[\"HYPER_PARAM\"][\"batch_size\"],\n","                                tokenizer, parsed_data[\"train\"]).torch_data()\n","        test_data = GeneralModelInput (config[\"HYPER_PARAM\"][\"sequence_length\"],\n","                               config[\"HYPER_PARAM\"][\"batch_size\"],\n","                               tokenizer, parsed_data[\"test\"]).torch_data()\n","    model = nn.DataParallel(model)\n","    opti = optim.Adam(model.parameters(), lr=config[\"HYPER_PARAM\"][\"learning_rate\"])\n","    criterion = nn.CrossEntropyLoss()\n","    #  TRAINING MODEL\n","    for ep in range(config[\"HYPER_PARAM\"][\"epochs\"]):\n","        print (\"-------------- EPOCH: \", ep, \" --------------\")\n","        print (\"\")\n","        for it, (seq, attn_masks, target_mask, labels) in enumerate(train_data):\n","            opti.zero_grad()\n","            seq, attn_masks, target_mask, labels = seq.cuda(0), attn_masks.cuda(0), target_mask.cuda(0), labels.cuda(0)\n","            logits = model(seq, attn_masks, target_mask)\n","            loss = criterion(logits, labels)\n","            loss.backward()\n","            opti.step()\n","\n","            if (it + 1) % 10 == 0:\n","                acc = accuracy_from_logits(logits, labels)\n","                print(\"Iteration {} of epoch {} complete. Loss : {} Accuracy : {}\".format(it + 1, ep + 1, loss.item(),\n","                                                                                          acc))\n","    #  SAVING MODEL\n","    print (\"saving model\")\n","    if save_model:\n","        torch.save(model.state_dict(), config[\"PATH\"][\"model_save_path\"]+\"/\"+target_dataset+\".pt\")\n","\n","    # EVALUATION\n","    model.eval()\n","    predictions, true_labels, embeddings, testset_acc = [], [], [], []\n","    for it, (seq, attn_masks, target_mask, labels) in enumerate(test_data):\n","\n","        seq, attn_masks, target_mask, labels = seq.cuda(0), attn_masks.cuda(0), target_mask.cuda(0), labels.cuda(0)\n","\n","        with torch.no_grad():\n","            logits = model(seq, attn_masks, target_mask)\n","        tst_acc = accuracy_from_logits(logits, labels)\n","        testset_acc.append(tst_acc)\n","        logits = list(np.argmax(logits.detach().cpu().numpy(), axis=1))\n","        label_ids = list(labels.to('cpu').numpy())\n","\n","\n","        predictions += logits\n","        true_labels += label_ids\n","\n","    total = sum(testset_acc)/len(testset_acc)\n","\n","    if model_name == \"bert-base-multilingual-uncased\":\n","        output_folder = \"multilingual\"\n","    elif model_name == \"DeepPavlov/rubert-base-cased\":\n","        output_folder = \"rubert\"\n","    else:\n","        output_folder = \"other_model\"\n","    with open (config[\"PATH\"][\"output_path\"]+\"/\" +\n","               output_folder +\"/\"+ target_dataset+\".txt\", \"w\") as out:\n","        for p in predictions:\n","            out.write(str(p)+\"\\n\")\n","    with open(config[\"PATH\"][\"output_path\"] + \"/\" + output_folder + \"/\" +target_dataset+ \"_acc.txt\", \"a\") as acc:\n","        acc.write(str(testset_acc))\n","        acc.write(str(total))"],"metadata":{"id":"H6EQgrQGkjRu"},"execution_count":null,"outputs":[]}]}